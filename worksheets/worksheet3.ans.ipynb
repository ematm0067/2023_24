{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "This week we will look at some unsupervised clustering algorithms. In this worksheet, we will start off by implementing $k$-means from scratch. We go on to look at using elbow plots to select a good value of $k$.\n",
    "\n",
    "We go on to complare the behaviour of k-means with hierarchical clustering and Gaussian mixture models.\n",
    "\n",
    "NB: if you find implementing the $k$-means algorithm form scratch challenging, **don't worry**! Have a go, but the rest of the worksheet is not dependent on doing this, so you can skip that question and come back to it when you finish the rest of the worksheet.\n",
    "\n",
    "# K-means clustering\n",
    "In the lecture, we saw that k-means is an unsupervised clustering algorithm. Recall that the algorithm runs as follows:\n",
    "\n",
    "Given a set of datapoints drawn from $\\Omega=\\mathbb{R}^n$:\n",
    "\n",
    "1. Randomly partition the set of datapoints into $k$ sets.\n",
    "2. For each set $P$ calculate its mean vector:\n",
    "$$\n",
    "\\hat{x}_P=\\left( \\frac{\\sum_{\\vec{x} \\in P} x_1}{|P|}, \\ldots,\\frac{\\sum_{\\vec{x} \\in P} x_i}{|P|} \\ldots, \\frac{\\sum_{\\vec{x} \\in P} x_n}{|P|}  \\right)\n",
    "$$\n",
    "3. For each datapoint evaluate the squared Euclidean distance from each of the mean vectors e.g. $||\\vec{x}-\\hat{x}_P ||^2$. Reallocate the datapoint to the partition set the mean of which it is closest to.\n",
    "4. If the partition sets remain unchanged then stop.  Else go to 2.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the $k$-means function from scikit-learn\n",
    "Scikit-learn has $k$-means built in. We import it using the command `from sklearn.cluster import KMeans`. Look at the documentation for KMeans (https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#). The KMeans estimator has 4 attributes. What are they?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The attributes are:\n",
    "1. `cluster_centers_`\n",
    "2. `labels_`\n",
    "3. `inertia_`\n",
    "4. `n_iter_`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which attribute would you use if you wanted to look at the labels assigned to the datapoints? What if you wanted to look at the centroids? What would you use the attribute `inertia_` for?\n",
    "\n",
    "A: `labels_`, `cluster_centers_`, measuring the within-cluster sum of squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following code creates some artificial data and plots it\n",
    "from sklearn.datasets import make_blobs\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "X, y = make_blobs(centers=3,n_samples=100, cluster_std=2, random_state=100)\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(X[:,0], X[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating elbow plots\n",
    "We will run $k$ means over our toy dataset for multiple values of $k$ and generate an elbow plot. To do this we can use the attribute `inertia_`. This attribute measures the within-cluster sum of squares, or the variance of each cluster, and the $k$ means algorithm works to minimize this quantity. The within-cluster sum of squares is defined as:\n",
    "$$\\sum_{j=1}^k\\sum_{x \\in P_j}||x - \\mu_j||^2$$\n",
    "To generate the elbow plot, we run $k$ means for values of $k$ from 1 to 10, and plot the inertia at each point. If there is a clear 'elbow' in the plot, then this gives us the optimal value of $k$. Do you see a clear 'elbow' in the plot? If so, what is the optimal value of $k$?\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import KMeans from sklearn.cluster\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Optional: write your own function to calculate the inertia \n",
    "# (otherwise you can just use the attribute inertia_)\n",
    "def inertia(X, labels, centroids):\n",
    "    distances=[np.linalg.norm(x - centroids[l])**2 for (x, l) in zip(X, labels)]\n",
    "    return np.sum(distances)\n",
    "        \n",
    "# Set up a variable to store the inertias\n",
    "inertias = []\n",
    "K = 10\n",
    "# Loop over values of k from 1 to 10\n",
    "for k in range(1, K+1):\n",
    "    # Instantiate the KMeans class with k clusters\n",
    "    kmm = KMeans(n_clusters=k)\n",
    "    # Fit the model to the data\n",
    "    kmm.fit(X) \n",
    "    # Store the value of the inertia for this value of k\n",
    "    inertias.append(kmm.inertia_)\n",
    "\n",
    "# Plot the elbow\n",
    "plt.figure()\n",
    "plt.plot(range(1, K+1), inertias, 'bx-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Inertia')\n",
    "plt.title('The elbow method showing the optimal k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering the iris dataset\n",
    "The Iris flower data set or Fisher's Iris data set is a multivariate data set introduced by the British statistician and biologist Ronald Fisher in 1936. The data set consists of 50 samples from each of three species of Iris (Iris setosa, Iris virginica and Iris versicolor). There are four features corresponding to the length and the width of the sepals and petals, in centimetres. Typically, the Iris data set is used as a classification problem, but by considering only the 4-D input feature space we can also apply clustering algorithms to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the iris dataset, and save the data into a variable X (take a look at the documentation here: \n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html)\n",
    "\n",
    "from sklearn import datasets\n",
    "iris = datasets.load_iris() \n",
    "X = iris.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin by assuming that since there are 3 types of iris, then there may be 3 clusters. Instantiate a $k$-means classifier with 3 clusters, and fit it to the data. Print out the centroids. You can visualise the resulting clusters by generating scatter plots projected on 2 dimensions. Try generating scatter plots for various combinations of features.\n",
    "\n",
    "**Extra question** Generate one large plot with subplots for each combination of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the iris dataset\n",
    "kmm = KMeans(n_clusters = 3)\n",
    "kmm.fit(X)\n",
    "# Make a scatter plot of the data on the first two axes\n",
    "# Experiment with looking at different axes\n",
    "plt.figure()\n",
    "plt.scatter(X[:,0],X[:,1], c=kmm.labels_, cmap='rainbow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot all combinations of data in one figure\n",
    "fig, axs = plt.subplots(4,4, figsize=(10,10), sharey='row', sharex='col')\n",
    "for i in range(4):\n",
    "    for j in range(4):\n",
    "        axs[j,i].scatter(X[:,i],X[:,j], c=kmm.labels_, cmap='rainbow')\n",
    "        if j == 3:\n",
    "            axs[j,i].set_xlabel(iris.feature_names[i])\n",
    "        if i == 0:\n",
    "            axs[j,i].set_ylabel(iris.feature_names[j])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate an elbow plot for this data set. To what extent does this elbow plot support the assumption that there are three clusters present in the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Generate an elbow plot for this dataset\n",
    "inertias = []\n",
    "K = 10 \n",
    "for k in range(1, K+1):\n",
    "    kmm = KMeans(n_clusters=k).fit(X) \n",
    "    kmm.fit(X) \n",
    "    inertias.append(kmm.inertia_)\n",
    "# Plot the elbow\n",
    "plt.figure()\n",
    "plt.plot(range(1, K+1), inertias, 'bx-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Inertia')\n",
    "plt.title('The elbow method showing the optimal k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The slope of the elbow does not change very abruptly. This means that there is no clear evidence whether two or three clusters is more appropriate for  this dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Implementing $k$-means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a function `kmeans` that takes a value $k$ and the data $X$, clusters the data, and returns the centroids and the labels of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import default_rng\n",
    "\n",
    "def kmeans(k, X):\n",
    "    # randomly assign labels to the data\n",
    "    rng = default_rng()\n",
    "    labels = np.array([i%k for i in range(len(X))])\n",
    "    new_labels = np.ones(len(X))\n",
    "    # set up a while loop that will run until the data labels no longer change\n",
    "    while True:\n",
    "        # Calculate the centroids of the data        \n",
    "        centres=np.array([np.sum(X[labels==ki], axis = 0)/len(X[labels==ki]) \\\n",
    "                          if len(X[labels==ki]) != 0 else centres[ki] for ki in range(k)])\n",
    "        # For each datapoint:\n",
    "        for i, x in enumerate(X):\n",
    "            #Calculate the squared Euclidean distance to each centroid\n",
    "            dist_to_centres = np.array([np.linalg.norm(x-a)**2 for a in centres])            \n",
    "            # Assign new labels based on distance to the centroid\n",
    "            new_labels[i] = np.argmin(list(dist_to_centres))\n",
    "        # If all the new labels are equal to the old labels, break out of the while loop    \n",
    "        if np.all(labels == new_labels):\n",
    "            break\n",
    "        # Assign the values of the new labels to the variable labels\n",
    "        labels[:]=new_labels\n",
    "    # return the centres and the labels.\n",
    "    return centres, labels\n",
    "\n",
    "# Plot the centroids on the data. Are they as you would expect?\n",
    "centres, labels = kmeans(3, X)\n",
    "ax.scatter(centres[:,0],centres[:,1])\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical clustering\n",
    "In this question we investigate the use of hierarchical clustering on the Iris data set. SciPy (pronounced 'Sigh Pie') is a Python-based ecosystem of open-source software for mathematics, science, and engineering. We start by importing packages `dendrogram` and `linkage`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram, linkage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following will generate a dendogram for the iris data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linked = linkage(X, 'single')\n",
    "labelList = range(len(X))\n",
    "plt.figure(figsize=(10, 7)) \n",
    "dendrogram(linked,labels=labelList) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall from the lectures that there are a number of ways of measuring the distance between clusters. For example:\n",
    "* Minimum distance: $d(S,T) = \\min\\{d(x,y) : x \\in S,y \\in T\\} $\n",
    "* Average distance: $d(S,T) = \\frac{1}{|S||T|} \\sum_{(x, y)} d(x, y)$\n",
    "* Maximum distance: $d(S,T) = \\max\\{d(x,y) : x \\in S,y \\in T\\} $\n",
    "* Centroid distance: $ d(S,T) = d(\\frac{\\sum_{x\\in S} x}{|S|} \\frac{\\sum_{y\\in T} y}{|T|})$\n",
    "\n",
    "The parameter `'single'` in linkage refers to minimum distance. This can be change to `'average'` for average distance, `'complete'` for maximum distance and `'centroid'` for centroid distance. Generate the dendogram for each of these cases. Comment on which metrics are most consistent with the assumption of 3 clusters in the iris data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs=plt.subplots(2,2, figsize=(15, 15))\n",
    "metrics=['single', 'average', 'complete',  'centroid']\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        linked=linkage(X, metrics[(i*2)+j])\n",
    "        dendrogram(linked, ax=axs[i,j])\n",
    "        axs[i,j].set_title(metrics[(i*2)+j])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The metric most consistent with the assumption of 3 clusters is: complete, since the distance between 3 clusters and more than 3 clusters is greatest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Mixture models\n",
    "In this question we investigate the use of Gaussian clustering on the Iris data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture as GMM\n",
    "gmm = GMM(n_components=3)\n",
    "gmm.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can extract the parameters for the learnt Gaussian distributions as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gmm.means_)\n",
    "print(gmm.covariances_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do the means for the three distributions compare with the centroids from a 3-cluster $k$-means on this dataset? \n",
    "\n",
    "A: One of the means is identical: that of the more isolated cluster. The others are close."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the iris dataset\n",
    "kmm = KMeans(n_clusters = 3)\n",
    "kmm.fit(X)\n",
    "print(kmm.cluster_centers_)\n",
    "# Make a scatter plot of the data on the first two axes\n",
    "# Experiment with looking at different axes\n",
    "plt.figure()\n",
    "plt.scatter(X[:,2],X[:,3], c=kmm.labels_, cmap='rainbow')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the command `print(gmm.weights_)` to look at the weights for each distribution. What do these weights tell us about the composition of the three clusters?\n",
    "\n",
    "A: The weight of the isolated cluster is 1/3, indicating that this cluster accounts for 1/3 of the points. The other clusters each account for just under and just over 1/3 of the points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gmm.weights_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
