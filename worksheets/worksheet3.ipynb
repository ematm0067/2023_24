{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQJijq5VQahk"
      },
      "source": [
        "# Introduction\n",
        "We will look at some unsupervised clustering algorithms. In this worksheet, we will start off by using $k$-means to cluster some data. We go on to look at using elbow plots to select a good value of $k$.\n",
        "\n",
        "Optionally, you can have a go at implementing $k$-means from scratch.\n",
        "\n",
        "We go on to complare the behaviour of $k$-means with hierarchical clustering and Gaussian mixture models.\n",
        "\n",
        "NB: if you find implementing the $k$-means algorithm form scratch challenging, **don't worry**! Have a go, but the rest of the worksheet is not dependent on doing this, so you can skip that question and come back to it when you finish the rest of the worksheet.\n",
        "\n",
        "# K-means clustering\n",
        "In the lecture, we saw that k-means is an unsupervised clustering algorithm. Recall that the algorithm runs as follows:\n",
        "\n",
        "Given a set of datapoints drawn from $V=\\mathbb{R}^n$:\n",
        "\n",
        "1. Randomly partition the set of datapoints into $k$ sets.\n",
        "2. For each set $P$ calculate its mean vector:\n",
        "$$\n",
        "\\hat{x}_P=\\left( \\frac{\\sum_{\\vec{x} \\in P} x_1}{|P|}, \\ldots,\\frac{\\sum_{\\vec{x} \\in P} x_i}{|P|} \\ldots, \\frac{\\sum_{\\vec{x} \\in P} x_n}{|P|}  \\right)\n",
        "$$\n",
        "3. For each datapoint evaluate the squared Euclidean distance from each of the mean vectors e.g. $||\\vec{x}-\\hat{x}_P ||^2$. Reallocate the datapoint to the partition set the mean of which it is closest to.\n",
        "4. If the partition sets remain unchanged then stop.  Else go to 2.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGLSWAjIQahn"
      },
      "source": [
        "## Using the $k$-means function from scikit-learn\n",
        "Scikit-learn has $k$-means built in. We import it using the command `from sklearn.cluster import KMeans`. Look at the documentation for KMeans (https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#). The KMeans estimator has 4 attributes. What are they?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KpH4XAraQahp"
      },
      "source": [
        "The attributes are:\n",
        "1.\n",
        "2.\n",
        "3.\n",
        "4."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzznYy82Qahq"
      },
      "source": [
        "Which attribute would you use if you wanted to look at the labels assigned to the datapoints? What if you wanted to look at the centroids? What would you use the attribute `inertia_` for?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ENJtDZlQahr"
      },
      "outputs": [],
      "source": [
        "# The following code creates some artificial data and plots it\n",
        "from sklearn.datasets import make_blobs\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "X, y = make_blobs(centers=3,n_samples=100, cluster_std=2, random_state=100)\n",
        "fig, ax = plt.subplots()\n",
        "ax.scatter(X[:,0], X[:,1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6BysTfRwQahs"
      },
      "source": [
        "### Generating elbow plots\n",
        "We will run $k$ means over our toy dataset for multiple values of $k$ and generate an elbow plot. To do this we can use the attribute `inertia_`. This attribute measures the within-cluster sum of squares, or the variance of each cluster, and the $k$ means algorithm works to minimize this quantity. The within-cluster sum of squares is defined as:\n",
        "$$\\sum_{j=1}^k\\sum_{x \\in P_j}||x - \\mu_j||^2$$\n",
        "To generate the elbow plot, we run $k$ means for values of $k$ from 1 to 10, and plot the inertia at each point. If there is a clear 'elbow' in the plot, then this gives us the optimal value of $k$. Do you see a clear 'elbow' in the plot? If so, what is the optimal value of $k$?\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AdLgVGMMQaht"
      },
      "outputs": [],
      "source": [
        "# Import KMeans from sklearn.cluster\n",
        "##TODO##\n",
        "\n",
        "# Optional: write your own function to calculate the inertia\n",
        "# (otherwise you can just use the attribute inertia_)\n",
        "def inertia(X, labels, centroids):\n",
        "    ##TODO## (Optional)\n",
        "\n",
        "# Set up a variable to store the inertias\n",
        "inertias = []\n",
        "\n",
        "# Loop over values of k from 1 to 10\n",
        "for k in range(1, K+1):\n",
        "    # Instantiate the KMeans class with k clusters\n",
        "    ##TODO##\n",
        "\n",
        "    # Fit the model to the data\n",
        "    ##TODO##\n",
        "\n",
        "    # Store the value of the inertia for this value of k\n",
        "    ##TODO##\n",
        "\n",
        "# Plot the elbow\n",
        "plt.figure()\n",
        "plt.plot(range(1, K+1), inertias, 'bx-')\n",
        "plt.xlabel('k')\n",
        "plt.ylabel('Inertia')\n",
        "plt.title('The elbow method showing the optimal k')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2p0RK4TOQahu"
      },
      "source": [
        "## (Optional) Implementing $k$-means"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7OniDOaQahv"
      },
      "source": [
        "(Optional) Implement a function `kmeans` that takes a value $k$ and the data $X$, clusters the data, and returns the centroids and the labels of the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MhCzmYx1Qahw"
      },
      "outputs": [],
      "source": [
        "def kmeans(k, X):\n",
        "    # randomly assign labels to the data\n",
        "    ##TODO##\n",
        "\n",
        "    # set up a while loop that will run until the data labels no longer change\n",
        "    while True:\n",
        "        # Calculate the centroids of the data\n",
        "        ##TODO##\n",
        "\n",
        "        # For each datapoint:\n",
        "        for i, x in enumerate(X):\n",
        "            #Calculate the squared Euclidean distance to each centroid\n",
        "            ##TODO##\n",
        "\n",
        "            # Assign new labels based on distance to the centroid\n",
        "            ##TODO##\n",
        "\n",
        "        # If all the new labels are equal to the old labels,\n",
        "        # break out of the while loop\n",
        "        ##TODO##\n",
        "\n",
        "        # Assign the values of the new labels to the variable labels\n",
        "        ##TODO##\n",
        "\n",
        "    # return the centres and the labels.\n",
        "    return centres, labels\n",
        "\n",
        "# Plot the centroids on the data. Are they as you would expect?\n",
        "centres, labels = kmeans(3, X)\n",
        "ax.scatter(centres[:,0],centres[:,1])\n",
        "fig"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5nwDX17eQahx"
      },
      "source": [
        "# Clustering the iris dataset\n",
        "The Iris flower data set or Fisher's Iris data set is a multivariate data set introduced by the British statistician and biologist Ronald Fisher in 1936. The data set consists of 50 samples from each of three species of Iris (Iris setosa, Iris virginica and Iris versicolor). There are four features corresponding to the length and the width of the sepals and petals, in centimetres. Typically, the Iris data set is used as a classification problem, but by considering only the 4-D input feature space we can also apply clustering algorithms to it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ylpp9bmoQahx"
      },
      "outputs": [],
      "source": [
        "# Import the iris dataset, and save the data into a variable X\n",
        "# (take a look at the documentation here:\n",
        "# https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html)\n",
        "##TODO##\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wIaUlUgiQahz"
      },
      "source": [
        "Let's begin by assuming that since there are 3 types of iris, then there may be 3 clusters. Instantiate a $k$-means classifier with 3 clusters, and fit it to the data. Print out the centroids. You can visualise the resulting clusters by generating scatter plots projected on 2 dimensions. Try generating scatter plots for various combinations of features.\n",
        "\n",
        "**Extra question** Generate one large plot with subplots for each combination of features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PK6-s_ScQah0"
      },
      "outputs": [],
      "source": [
        "# Fit the iris dataset\n",
        "##TODO##\n",
        "\n",
        "# Make a scatter plot of the data on the first two axes\n",
        "# Experiment with looking at different axes\n",
        "##TODO##\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F6bkwd5rQah1"
      },
      "outputs": [],
      "source": [
        "# Optional: Plot all combinations of data in one large plot with subplots for each combination of features\n",
        "##TODO##\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2RDEWE7CQah1"
      },
      "source": [
        "Generate an elbow plot for this data set. To what extent does this elbow plot support the assumption that there are three clusters present in the data?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ukzWe62Qah2"
      },
      "outputs": [],
      "source": [
        "# Generate an elbow plot for this dataset\n",
        "##TODO##\n",
        "\n",
        "# Plot the elbow\n",
        "##TODO##\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zI2RTfQXQah2"
      },
      "source": [
        "# Hierarchical clustering\n",
        "In this question we investigate the use of hierarchical clustering on the Iris data set. SciPy (pronounced 'Sigh Pie') is a Python-based ecosystem of open-source software for mathematics, science, and engineering. We start by importing packages `dendrogram` and `linkage`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zO03UqpmQah2"
      },
      "outputs": [],
      "source": [
        "from scipy.cluster.hierarchy import dendrogram, linkage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMlwfwSKQah3"
      },
      "source": [
        "The following will generate a dendogram for the iris data set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "igGhNml1Qah3"
      },
      "outputs": [],
      "source": [
        "linked = linkage(X, 'single')\n",
        "labelList = range(len(X))\n",
        "plt.figure(figsize=(10, 7))\n",
        "dendrogram(linked,labels=labelList)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LnR5BUp9Qah4"
      },
      "source": [
        "Recall from the lectures that there are a number of ways of measuring the distance between clusters. For example:\n",
        "* Minimum distance: $d(S,T) = \\min\\{d(x,y) : x \\in S,y \\in T\\} $\n",
        "* Average distance: $d(S,T) = \\frac{1}{|S||T|} \\sum_{(x, y)} d(x, y)$\n",
        "* Maximum distance: $d(S,T) = \\max\\{d(x,y) : x \\in S,y \\in T\\} $\n",
        "* Centroid distance: $ d(S,T) = d(\\frac{\\sum_{x\\in S} x}{|S|} \\frac{\\sum_{y\\in T} y}{|T|})$\n",
        "\n",
        "The parameter `'single'` in linkage refers to minimum distance. This can be change to `'average'` for average distance, `'complete'` for maximum distance and `'centroid'` for centroid distance. Generate the dendogram for each of these cases. Comment on which metrics are most consistent with the assumption of 3 clusters in the iris data set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RkWz4rA1Qah4"
      },
      "outputs": [],
      "source": [
        "# Generate dendrograms for each distance metric\n",
        "##TODO##"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_CY7UTPQah5"
      },
      "source": [
        "The metrics most consistent with the assumption of 3 clusters are:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YGmy79g_Qah5"
      },
      "source": [
        "# Gaussian Mixture models\n",
        "In this question we investigate the use of Gaussian clustering on the Iris data set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ICXbjIxQah5"
      },
      "outputs": [],
      "source": [
        "from sklearn.mixture import GaussianMixture as GMM\n",
        "gmm = GMM(n_components=3)\n",
        "gmm.fit(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6QqAZfEZQah6"
      },
      "source": [
        "We can extract the parameters for the learnt Gaussian distributions as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RdnVDuO-Qah6"
      },
      "outputs": [],
      "source": [
        "print(gmm.means_)\n",
        "print(gmm.covariances_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cdv34GDeQah6"
      },
      "source": [
        "How do the means for the three distributions compare with the centroids from a 3-cluster $k$-means on this dataset?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jevek3ikQah7"
      },
      "outputs": [],
      "source": [
        "# Compare the means from the GMM clusters with the means from\n",
        "# the k-means clusters\n",
        "##TODO##"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1entJ-nQah7"
      },
      "source": [
        "Use the command `print(gmm.weights_)` to look at the weights for each distribution. What do these weights tell us about the composition of the three clusters?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xSuDZSl_Qah7"
      },
      "outputs": [],
      "source": [
        "##TODO##"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xGMjEs7kQah8"
      },
      "source": [
        "Generate scatter plots for different 2-D combinations of the features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5asPF2iBQah8"
      },
      "outputs": [],
      "source": [
        "##TODO##"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tUpwWkHBQah9"
      },
      "source": [
        "## Using the $k$-means function from scikit-learn\n",
        "Scikit-learn has $k$-means built in. We import it using the command `from sklearn.cluster import KMeans`. Look at the documentation for KMeans (https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#). The KMeans estimator has 4 attributes. What are they?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfgXrVIAQah9"
      },
      "source": [
        "The attributes are:\n",
        "1.\n",
        "2.\n",
        "3.\n",
        "4."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_Rz83kBQah9"
      },
      "source": [
        "Which attribute would you use if you wanted to look at the labels assigned to the datapoints? What if you wanted to look at the centroids? What would you use the attribute `inertia_` for?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "56rzc8BMQah9"
      },
      "outputs": [],
      "source": [
        "# The following code creates some artificial data and plots it\n",
        "from sklearn.datasets import make_blobs\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "X, y = make_blobs(centers=3,n_samples=100, cluster_std=2, random_state=100)\n",
        "fig, ax = plt.subplots()\n",
        "ax.scatter(X[:,0], X[:,1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHOR0bGEQah-"
      },
      "source": [
        "### Generating elbow plots\n",
        "We will run $k$ means over our toy dataset for multiple values of $k$ and generate an elbow plot. To do this we can use the attribute `inertia_`. This attribute measures the within-cluster sum of squares, or the variance of each cluster, and the $k$ means algorithm works to minimize this quantity. The within-cluster sum of squares is defined as:\n",
        "$$\\sum_{j=1}^k\\sum_{x \\in P_j}||x - \\mu_j||^2$$\n",
        "To generate the elbow plot, we run $k$ means for values of $k$ from 1 to 10, and plot the inertia at each point. If there is a clear 'elbow' in the plot, then this gives us the optimal value of $k$. Do you see a clear 'elbow' in the plot? If so, what is the optimal value of $k$?\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nE68JtL8Qah-"
      },
      "outputs": [],
      "source": [
        "# Import KMeans from sklearn.cluster\n",
        "##TODO##\n",
        "\n",
        "# Optional: write your own function to calculate the inertia\n",
        "# (otherwise you can just use the attribute inertia_)\n",
        "def inertia(X, labels, centroids):\n",
        "    ##TODO## (Optional)\n",
        "\n",
        "# Set up a variable to store the inertias\n",
        "inertias = []\n",
        "\n",
        "# Loop over values of k from 1 to 10\n",
        "for k in range(1, K+1):\n",
        "    # Instantiate the KMeans class with k clusters\n",
        "    ##TODO##\n",
        "\n",
        "    # Fit the model to the data\n",
        "    ##TODO##\n",
        "\n",
        "    # Store the value of the inertia for this value of k\n",
        "    ##TODO##\n",
        "\n",
        "# Plot the elbow\n",
        "plt.figure()\n",
        "plt.plot(range(1, K+1), inertias, 'bx-')\n",
        "plt.xlabel('k')\n",
        "plt.ylabel('Inertia')\n",
        "plt.title('The elbow method showing the optimal k')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uX9mMwzPQah-"
      },
      "source": [
        "## (Optional) Implementing $k$-means"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ef5TdVIhQah_"
      },
      "source": [
        "(Optional) Implement a function `kmeans` that takes a value $k$ and the data $X$, clusters the data, and returns the centroids and the labels of the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aTOHEgCEQah_"
      },
      "outputs": [],
      "source": [
        "def kmeans(k, X):\n",
        "    # randomly assign labels to the data\n",
        "    ##TODO##\n",
        "\n",
        "    # set up a while loop that will run until the data labels no longer change\n",
        "    while True:\n",
        "        # Calculate the centroids of the data\n",
        "        ##TODO##\n",
        "\n",
        "        # For each datapoint:\n",
        "        for i, x in enumerate(X):\n",
        "            #Calculate the squared Euclidean distance to each centroid\n",
        "            ##TODO##\n",
        "\n",
        "            # Assign new labels based on distance to the centroid\n",
        "            ##TODO##\n",
        "\n",
        "        # If all the new labels are equal to the old labels,\n",
        "        # break out of the while loop\n",
        "        ##TODO##\n",
        "\n",
        "        # Assign the values of the new labels to the variable labels\n",
        "        ##TODO##\n",
        "\n",
        "    # return the centres and the labels.\n",
        "    return centres, labels\n",
        "\n",
        "# Plot the centroids on the data. Are they as you would expect?\n",
        "centres, labels = kmeans(3, X)\n",
        "ax.scatter(centres[:,0],centres[:,1])\n",
        "fig"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJ0wcfOxQah_"
      },
      "source": [
        "# Clustering the iris dataset\n",
        "The Iris flower data set or Fisher's Iris data set is a multivariate data set introduced by the British statistician and biologist Ronald Fisher in 1936. The data set consists of 50 samples from each of three species of Iris (Iris setosa, Iris virginica and Iris versicolor). There are four features corresponding to the length and the width of the sepals and petals, in centimetres. Typically, the Iris data set is used as a classification problem, but by considering only the 4-D input feature space we can also apply clustering algorithms to it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zJnFGCeKQah_"
      },
      "outputs": [],
      "source": [
        "# Import the iris dataset, and save the data into a variable X\n",
        "# (take a look at the documentation here:\n",
        "# https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html)\n",
        "##TODO##\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rnM7NKm7QaiA"
      },
      "source": [
        "Let's begin by assuming that since there are 3 types of iris, then there may be 3 clusters. Instantiate a $k$-means classifier with 3 clusters, and fit it to the data. Print out the centroids. You can visualise the resulting clusters by generating scatter plots projected on 2 dimensions. Try generating scatter plots for various combinations of features.\n",
        "\n",
        "**Extra question** Generate one large plot with subplots for each combination of features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HD8Z5eyrQaiA"
      },
      "outputs": [],
      "source": [
        "# Fit the iris dataset\n",
        "##TODO##\n",
        "\n",
        "# Make a scatter plot of the data on the first two axes\n",
        "# Experiment with looking at different axes\n",
        "##TODO##\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7CVbFOQ_QaiB"
      },
      "outputs": [],
      "source": [
        "# Optional: Plot all combinations of data in one large plot with subplots for each combination of features\n",
        "##TODO##\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNWSCChVQaiB"
      },
      "source": [
        "Generate an elbow plot for this data set. To what extent does this elbow plot support the assumption that there are three clusters present in the data?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LS7NHSCkQaiB"
      },
      "outputs": [],
      "source": [
        "# Generate an elbow plot for this dataset\n",
        "##TODO##\n",
        "\n",
        "# Plot the elbow\n",
        "##TODO##\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZARC8UgJQaiC"
      },
      "source": [
        "# Hierarchical clustering\n",
        "In this question we investigate the use of hierarchical clustering on the Iris data set. SciPy (pronounced 'Sigh Pie') is a Python-based ecosystem of open-source software for mathematics, science, and engineering. We start by importing packages `dendrogram` and `linkage`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "THZ79oPDQaiC"
      },
      "outputs": [],
      "source": [
        "from scipy.cluster.hierarchy import dendrogram, linkage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9yxblunQaiD"
      },
      "source": [
        "The following will generate a dendogram for the iris data set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LkIrCxGtQaiD"
      },
      "outputs": [],
      "source": [
        "linked = linkage(X, 'single')\n",
        "labelList = range(len(X))\n",
        "plt.figure(figsize=(10, 7))\n",
        "dendrogram(linked,labels=labelList)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_uKoxIGDQaiE"
      },
      "source": [
        "Recall from the lectures that there are a number of ways of measuring the distance between clusters. For example:\n",
        "* Minimum distance: $d(S,T) = \\min\\{d(x,y) : x \\in S,y \\in T\\} $\n",
        "* Average distance: $d(S,T) = \\frac{1}{|S||T|} \\sum_{(x, y)} d(x, y)$\n",
        "* Maximum distance: $d(S,T) = \\max\\{d(x,y) : x \\in S,y \\in T\\} $\n",
        "* Centroid distance: $ d(S,T) = d(\\frac{\\sum_{x\\in S} x}{|S|} \\frac{\\sum_{y\\in T} y}{|T|})$\n",
        "\n",
        "The parameter `'single'` in linkage refers to minimum distance. This can be change to `'average'` for average distance, `'complete'` for maximum distance and `'centroid'` for centroid distance. Generate the dendogram for each of these cases. Comment on which metrics are most consistent with the assumption of 3 clusters in the iris data set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nZ01ryl-QaiE"
      },
      "outputs": [],
      "source": [
        "# Generate dendrograms for each distance metric\n",
        "##TODO##"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSQ05r70QaiF"
      },
      "source": [
        "The metrics most consistent with the assumption of 3 clusters are:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVBKUj19QaiF"
      },
      "source": [
        "# Gaussian Mixture models\n",
        "In this question we investigate the use of Gaussian clustering on the Iris data set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SteN_QHUQaiG"
      },
      "outputs": [],
      "source": [
        "from sklearn.mixture import GaussianMixture as GMM\n",
        "gmm = GMM(n_components=3)\n",
        "gmm.fit(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JX_YPsOdQaiG"
      },
      "source": [
        "We can extract the parameters for the learnt Gaussian distributions as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rlw1I2rEQaiH"
      },
      "outputs": [],
      "source": [
        "print(gmm.means_)\n",
        "print(gmm.covariances_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3YBgOmKjQaiH"
      },
      "source": [
        "How do the means for the three distributions compare with the centroids from a 3-cluster $k$-means on this dataset?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NJn2EZ3zQaiH"
      },
      "outputs": [],
      "source": [
        "# Compare the means from the GMM clusters with the means from\n",
        "# the k-means clusters\n",
        "##TODO##"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pp_Y3J9SQaiI"
      },
      "source": [
        "Use the command `print(gmm.weights_)` to look at the weights for each distribution. What do these weights tell us about the composition of the three clusters?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JxaV1KzyQaiI"
      },
      "outputs": [],
      "source": [
        "##TODO##"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMSrUTa0QaiI"
      },
      "source": [
        "Generate scatter plots for different 2-D combinations of the features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gXmMpNdzQaiI"
      },
      "outputs": [],
      "source": [
        "##TODO##"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5 (default, Sep  4 2020, 02:22:02) \n[Clang 10.0.0 ]"
    },
    "vscode": {
      "interpreter": {
        "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
      }
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}